{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will automatically generate a set of evaluation questions based on wandb docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import json\n",
    "import wandb\n",
    "from wandb.sdk.lib.runid import generate_id\n",
    "from wandb.integration.langchain import WandbTracer\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import HypotheticalDocumentEmbedder, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.chains import HypotheticalDocumentEmbedder, RetrievalQAWithSourcesChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/wandb/docodile.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"langchain>=0.0.175\" \"wandb>=0.15.3\" openai cohere tqdm fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"wandbot_synth\" \n",
    "ENTITY = \"wandbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter password in the VS Code prompt at the top of your VS Code window!\n",
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "def get_openai_key():\n",
    "  if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "      print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "  assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "  print(\"OpenAI API key configured\")\n",
    "\n",
    "get_openai_key()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Questions with WandBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreRetrieverWithScore(VectorStoreRetriever):\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        if self.search_type == \"similarity\":\n",
    "            docs_and_scores = self.vectorstore.similarity_search_with_score(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "            docs = []\n",
    "            for doc, score in docs_and_scores:\n",
    "                doc.metadata[\"score\"] = score\n",
    "                docs.append(doc)\n",
    "        elif self.search_type == \"mmr\":\n",
    "            docs = self.vectorstore.max_marginal_relevance_search(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n",
    "        return docs\n",
    "\n",
    "\n",
    "class FAISSWithScore(FAISS):\n",
    "    def as_retriever(self) -> VectorStoreRetrieverWithScore:\n",
    "        return VectorStoreRetrieverWithScore(\n",
    "            vectorstore=self,\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 10},\n",
    "        )\n",
    "\n",
    "\n",
    "class RetrievalQAWithSourcesChainWithScore(RetrievalQAWithSourcesChain):\n",
    "    reduce_k_below_max_tokens: bool = True\n",
    "    max_tokens_limit: int = 2816\n",
    "\n",
    "    def _get_docs(self, inputs: Dict[str, Any]) -> List[Document]:\n",
    "        question = inputs[self.question_key]\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        return self._reduce_tokens_below_limit(docs)\n",
    "\n",
    "\n",
    "def load_artifacts(config):\n",
    "    faiss_artifact = wandb.use_artifact(config.faiss_artifact, type=\"search_index\")\n",
    "    faiss_artifact_dir = faiss_artifact.download()\n",
    "\n",
    "    hyde_prompt_artifact = wandb.use_artifact(\n",
    "        config.hyde_prompt_artifact, type=\"prompt\"\n",
    "    )\n",
    "    hyde_artifact_dir = hyde_prompt_artifact.download()\n",
    "    hyde_prompt_file = f\"{hyde_artifact_dir}/hyde_prompt.txt\"\n",
    "\n",
    "    chat_prompt_artifact = wandb.use_artifact(\n",
    "        config.chat_prompt_artifact, type=\"prompt\"\n",
    "    )\n",
    "    chat_artifact_dir = chat_prompt_artifact.download()\n",
    "    chat_prompt_file = f\"{chat_artifact_dir}/chat_prompt.txt\"\n",
    "\n",
    "    return {\n",
    "        \"faiss\": faiss_artifact_dir,\n",
    "        \"hyde_prompt\": hyde_prompt_file,\n",
    "        \"chat_prompt\": chat_prompt_file,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def parse_source_documents(source_documents):\n",
    "    source_docs_dict = {}\n",
    "    for i,source_doc in enumerate(source_documents):\n",
    "        source_docs_dict[f\"source_doc_{i}\"] ={\n",
    "            \"page_content\":source_doc.page_content,\n",
    "            \"metadata\":source_doc.metadata[\"source\"],\n",
    "            \"lookup_index\":source_doc.lookup_index,\n",
    "            \"lookup_str\":source_doc.lookup_str,\n",
    "            }   \n",
    "\n",
    "    return json.dumps(source_docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_chain.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# login to openai with your api key\n",
    "get_openai_key()\n",
    "\n",
    "wandbot_config = SimpleNamespace(\n",
    "    faiss_artifact=\"parambharat/wandb_docs_bot/faiss_store:latest\",\n",
    "    hyde_prompt_artifact=\"parambharat/wandb_docs_bot/hyde_prompt:latest\",\n",
    "    chat_prompt_artifact=\"parambharat/wandb_docs_bot/system_prompt:latest\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    eval_model = \"gpt-3.5-turbo\", #'command-nightly',\n",
    "    temperature=0,\n",
    "    hyde_llm_temperature=0.3,\n",
    "    openai_eval_llm_temperature=0.0,\n",
    "    command_llm_temperature=0.0,\n",
    "    cohere_generate_cost_usd = 0.0000025  # cost per characters (not tokens), $0.0025 per generation unit (1000 chars)\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "        name=\"synth_answer_generation_final\",\n",
    "        project=PROJECT,\n",
    "        entity=ENTITY,\n",
    "        config=wandbot_config,\n",
    "        )\n",
    "\n",
    "artifacts = load_artifacts(wandb.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load wandbot v1 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA AND PROMPTS FROM ARTIFACTS\n",
    "faiss_dir = artifacts[\"faiss\"]\n",
    "hyde_prompt_template =  open(artifacts[\"hyde_prompt\"]).read()\n",
    "wandbot_v1_system_prompt_template = open(artifacts[\"chat_prompt\"]).read()\n",
    "human_message_prompt_template = \"{question}\"\n",
    "\n",
    "\n",
    "# SETUP Hypothetical Document Embedder (HyDE)\n",
    "hyde_messages = [\n",
    "    SystemMessagePromptTemplate.from_template(hyde_prompt_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "]\n",
    "hyde_prompt = ChatPromptTemplate.from_messages(hyde_messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative System Prompts\n",
    "\n",
    "Create alternate wandbot prompts to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = {}\n",
    "\n",
    "system_prompts[\"wandbot_v1_few_shot\"] = wandbot_v1_system_prompt_template\n",
    "\n",
    "system_prompts[\"wandbot_v1_zero_shot\"]= \"\"\"\n",
    "As an AI assistant for the open source library wandb, your task is to answer questions based on \n",
    "the given extracted parts of a long document and the question. You can provide a conversational \n",
    "answer with a hyperlink to the documentation only if it is explicitly listed as a source in the context. \n",
    "\n",
    "Provide a code block directly from the documentation wherever possible. If you do not know the answer, \n",
    "you can say \"Hmm, I'm not sure.\" If the question is not related to wandb or Weights & Biases, politely \n",
    "inform the user that you can only answer questions related to wandb. The documentation for wandb can be\n",
    "found at https://docs.wandb.ai.\n",
    "\n",
    "Begin:\n",
    "================\n",
    "\n",
    "Question: {question}\n",
    "================\n",
    "{summaries}\n",
    "================\n",
    "Final Answer in Markdown:\n",
    "\"\"\"\n",
    "\n",
    "system_prompts[\"default_langchain_qa\"]= \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, \n",
    "just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompt token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandbot_v1_few_shot token count: 660\n",
      "wandbot_v1_zero_shot token count: 165\n",
      "default_langchain_qa token count: 54\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "jj = enc.encode(\"hello world\")\n",
    "len(jj)\n",
    "\n",
    "for k in system_prompts.keys():\n",
    "    print(f\"{k} token count: {len(enc.encode(system_prompts[k]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Propmt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_eval_system_prompt = \"\"\"As an experienced software quality assurance tester, you are evaluating the quality of the response (WANDBOT_RESPONSE) from a\n",
    "Weights & Biasses (aka wandb, W&B) support bot called `wandbot`. Weights & Biasses is a machine learning ops (MLOps) python library and app.\n",
    "Supporting documentation (SUPPORTING_DOCUMENTATION) is provided to help you assess the quality of the response. You job is to grade (GRADE) the response.\n",
    "\n",
    "Grade the following WANDBOT_RESPONSE given the USER_QUESTION and SUPPORTING_DOCUMENTATION. \n",
    "Grade the WANDBOT_RESPONSE based ONLY on its factual accuracy. It is OK if the WANDBOT_RESPONSE contains more information than in SUPPORTING_DOCUMENTATION, as long as it does not contain any conflicting statements.\n",
    "Your GRADE should only be POSITIVE or NEGATIVE to indicate whether the WANDBOT_RESPONSE is accurate or not given the SUPPORTING_DOCUMENTATION.\n",
    "If WANDBOT_RESPONSE says that there is no specific information provided in the context or that it doesn't know, then the GRADE is NEGATIVE.\n",
    "Be sure to read the SUPPORTING_DOCUMENTATION carefully before grading the WANDBOT_RESPONSE and be highlighly critical of the WANDBOT_RESPONSE.\n",
    "Only respond with POSITIVE or NEGATIVE for GRADE. \n",
    "In GRADE_JUSTIFICATION, explain why your GRADE of WANDBOT_RESPONSE is POSITIVE or NEGATIVE. Use one or two sentences maximum. Keep the GRADE_JUSTIFICATION as concise as possible.\n",
    "Remember, you are only grading the WANDBOT_RESPONSE. The SUPPORTING_DOCUMENTATION is only provided to help you grade the WANDBOT_RESPONSE.\n",
    "\n",
    "This is the format of the input USER_QUESTION, WANDBOT_RESPONSE and SUPPORTING_DOCUMENTATION that you will be given as well as the format of the GRADE_JUSTIFICATION and GRADE that you will provide:\n",
    "\n",
    "\n",
    "USER_QUESTION: user question here\n",
    "WANDBOT_RESPONSE: the response from the `wandbot` support bot here\n",
    "SUPPORTING_DOCUMENTATION: retrieved documentation from the wandb docs here\n",
    "\n",
    "\n",
    "GRADE_JUSTIFICATION: justification for the GRADE here, keep it concise, 1 or 2 sentences only\n",
    "GRADE: POSITIVE or NEGATIVE here\n",
    "\n",
    "\n",
    "Remember, start your reponse with \"GRADE_JUSTIFICATION\". You must also ways provide a \"GRADE\". Begin!\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Remember, you are only grading the WANDBOT_RESPONSE. The SUPPORTING_DOCUMENTATION is only provided to help you grade the WANDBOT_RESPONSE.\" \n",
    "- neagtive grade based on what was in supporting docs, even though response was correct\n",
    "\n",
    "\"You must always provide both a GRADE_JUSTIFICATION and a GRADE\" - missing GRADE\n",
    "\n",
    "\"Remember, start your reponse with \"GRADE_JUSTIFICATION\". You must also ways provide a \"GRADE\".\" and added \"GRADE_JUSTIFICATION:\" to the end of the human template - it started to reply with \"USER_QUESTION:...\" etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_human = \"\"\"\n",
    "\n",
    "USER_QUESTION: How do I create a wandb sweep?\n",
    "WANDBOT_RESPONSE: To create a W&B Articfact, you can use the wandb.Artifact class like so ```artifact = wandb.Artifact(name='bicycle-dataset', type='dataset')```\n",
    "SUPPORTING_DOCUMENTATION: '{{\"source_doc_0\": {{\"page_content\": \"Use Weights & Biases Sweeps to automate hyperparameter search and explore the space of possible models. Create a sweep with a few lines of code.\", \"metadata\": \"https://docs.wandb.ai/guide\", \"lookup_index\": 0, \"lookup_str\": \"\"}}}}'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example_assistant = \"\"\"\n",
    "GRADE_JUSTIFICATION: The USER_QUESTION was about how to create a wandb sweep, but the WANDBOT_RESPONSE answers how to create a wandb artifact. Therefore the WANDBOT_RESPONSE is not accurate and I will give a negative grade.\n",
    "GRADE: NEGATIVE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "eval_human_message_prompt_template = \"\"\"\n",
    "\n",
    "USER_QUESTION: {question}\n",
    "WANDBOT_RESPONSE: {answer}\n",
    "SUPPORTING_DOCUMENTATION: {source_documents}.\n",
    "\n",
    "\n",
    "GRADE_JUSTIFICATION:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_messages = [\n",
    "    SystemMessagePromptTemplate.from_template(openai_eval_system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(example_human, additional_kwargs={\"name\": \"example_user\"}),\n",
    "    AIMessagePromptTemplate.from_template(example_assistant, additional_kwargs={\"name\": \"example_assistant\"}),\n",
    "    HumanMessagePromptTemplate.from_template(eval_human_message_prompt_template)\n",
    "]\n",
    "eval_prompt = ChatPromptTemplate.from_messages(eval_messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_embeddings = OpenAIEmbeddings()\n",
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=LLMChain(llm=ChatOpenAI(\n",
    "        temperature=wandb.config.hyde_llm_temperature), \n",
    "        prompt=hyde_prompt),\n",
    "    base_embeddings=base_embeddings,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# LOAD FAISS VECTOR STORE\n",
    "vector_store = FAISSWithScore.load_local(faiss_dir, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD QA CHAINS FOR EACH SYSTEM PROMPT\n",
    "def load_qa_chain(system_prompt_template, vector_store=vector_store, chain_type=\"stuff\"):\n",
    "    qa_messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_template, input_variables=[\"context\", \"question\"]),\n",
    "        HumanMessagePromptTemplate.from_template(human_message_prompt_template),\n",
    "    ]   \n",
    "    qa_prompt = ChatPromptTemplate.from_messages(qa_messages)\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=wandb.config.model_name,\n",
    "        temperature=wandb.config.temperature,\n",
    "        request_timeout=120\n",
    "        )\n",
    "\n",
    "    qa_chain = RetrievalQAWithSourcesChainWithScore.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type=chain_type,\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": qa_prompt},\n",
    "        return_source_documents=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    return qa_chain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_timestamps(n=10000, start_date='2023-03-01', end_date='2023-05-31'):\n",
    "    # Range of datetimes with 1-minute intervals\n",
    "    rng = pd.date_range(start_date, end_date, freq='S')\n",
    "\n",
    "    # Create weights for all datetimes\n",
    "    weights = pd.Series(1, index=rng)\n",
    "\n",
    "    # Decrease weights for weekends\n",
    "    weights[rng.to_series().dt.dayofweek > 4] *= 0.5\n",
    "\n",
    "    # Decrease weight for Easter Sunday (2023-04-09)\n",
    "    easter = pd.to_datetime('2023-04-09')\n",
    "    weights[rng.to_series().between(easter, easter + pd.DateOffset(days=1))] *= 0.5\n",
    "    easter_monday = pd.to_datetime('2023-04-10')\n",
    "    weights[rng.to_series().between(easter_monday, easter_monday + pd.DateOffset(days=1))] *= 0.5\n",
    "\n",
    "    # Increase weights for 8am-6pm on weekdays\n",
    "    mask = ((rng.to_series().dt.hour >= 8) & (rng.to_series().dt.hour <= 18) & (rng.to_series().dt.dayofweek <= 4))\n",
    "    weights[mask] *= 1.2\n",
    "\n",
    "    # Increase weights for Tuesday, Wednesday, Thursday\n",
    "    mask = ((rng.to_series().dt.dayofweek >= 1) & (rng.to_series().dt.dayofweek <= 3))\n",
    "    weights[mask] *= 1.5\n",
    "\n",
    "    # Normalize weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Sample 10000 datetimes using weights\n",
    "    sampled_datetimes = np.random.choice(rng, size=n, p=weights)\n",
    "\n",
    "    # Sort the datetimes\n",
    "    sampled_datetimes.sort()\n",
    "\n",
    "    return sampled_datetimes\n",
    "\n",
    "timestamps = generate_timestamps()\n",
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_timestamps(timestamps):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(timestamps, bins=100, alpha=0.5, color='blue')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Timestamps')\n",
    "    plt.show()\n",
    "\n",
    "timestamps = generate_timestamps()\n",
    "timestamps_list = [t for t in timestamps]\n",
    "# timestamps_list[0]\n",
    "\n",
    "plot_timestamps(timestamps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WANDB LOGGING CONFIG\n",
    "wandb_config = {\"project\": PROJECT, \"entity\":ENTITY}  # config for OpenAI autologger\n",
    "\n",
    "table_cols = [\n",
    "  \"request_timestamp\",\"request_id\", \"question\", \"question_id\", \"wandbot_answer\", \"retrived_source_documents\",\n",
    "  \"answer_elapsed_time_s\", \"synth_grader_feedback\", \"grader_justification\", # \"eval_completion\",\n",
    "  \"prompt_tokens\", \"completion_tokens\", \"total_tokens\",\n",
    "  \"answer_cost_usd\", \"successful_requests\", \"answer_system_prompt_version\",\n",
    "  \"eval_elapsed_time_s\", \"eval_cost_usd\",  \"eval_total_tokens\", \"eval_tokens_for_source_docs\", \"eval_prompt_tokens\", \"eval_completion_tokens\", \"is_source_documents_truncated\",\n",
    "  # \"system_prompt_template\", \"human_message_prompt_template\", \"hyde_prompt_template\", \"eval_prompt_template\",\n",
    "  \"wandb_run_id\", \"wandbot_model\", \"wandbot_temperature\", \"hyde_llm_temperature\", \n",
    "  \"eval_model\", \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# # run = wandb.init()\n",
    "# artifact = wandb.use_artifact('wandbot/wandbot_synth/run-g4rjeq92-logsqa_with_eval_final2:latest', type='run_table')\n",
    "# df = artifact.get(\"logs/qa_with_eval_final2\").get_dataframe()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9670\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's start!\\n                Please generate ...</td>\n",
       "      <td>../docodile/docs/guides/artifacts/intro.md</td>\n",
       "      <td>Can one run use another run's output Artifact ...</td>\n",
       "      <td>3280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let's start!\\n                Please generate ...</td>\n",
       "      <td>../docodile/docs/ref/cli/wandb-sync.md</td>\n",
       "      <td>How can you specify the project you want to up...</td>\n",
       "      <td>8227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let's start!\\n                Please generate ...</td>\n",
       "      <td>../docodile/docs/ref/cli/wandb-login.md</td>\n",
       "      <td>What is the maximum number of authentication k...</td>\n",
       "      <td>8322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let's start!\\n                Please generate ...</td>\n",
       "      <td>../docodile/docs/ref/cli/wandb-pull.md</td>\n",
       "      <td>Is Wandb Pull only used for downloading files ...</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's start!\\n                Please generate ...</td>\n",
       "      <td>../docodile/docs/guides/sweeps/visualize-sweep...</td>\n",
       "      <td>What kind of behavior can be altered in the Ed...</td>\n",
       "      <td>6993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Let's start!\\n                Please generate ...   \n",
       "1  Let's start!\\n                Please generate ...   \n",
       "2  Let's start!\\n                Please generate ...   \n",
       "3  Let's start!\\n                Please generate ...   \n",
       "4  Let's start!\\n                Please generate ...   \n",
       "\n",
       "                                            document  \\\n",
       "0         ../docodile/docs/guides/artifacts/intro.md   \n",
       "1             ../docodile/docs/ref/cli/wandb-sync.md   \n",
       "2            ../docodile/docs/ref/cli/wandb-login.md   \n",
       "3             ../docodile/docs/ref/cli/wandb-pull.md   \n",
       "4  ../docodile/docs/guides/sweeps/visualize-sweep...   \n",
       "\n",
       "                                            question  question_id  \n",
       "0  Can one run use another run's output Artifact ...         3280  \n",
       "1  How can you specify the project you want to up...         8227  \n",
       "2  What is the maximum number of authentication k...         8322  \n",
       "3  Is Wandb Pull only used for downloading files ...         2683  \n",
       "4  What kind of behavior can be altered in the Ed...         6993  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.use_artifact('wandbot/wandbot_synth/run-2cv1ao9n-generated_questions_table:v0', type='run_table')\n",
    "# artifact_dir = artifact.download(\"data\")\n",
    "df = artifact.get(\"generated_questions_table\").get_dataframe()\n",
    "df[\"question_id\"] = df.index\n",
    "\n",
    "# SHUFFLE THE QUESTIONS\n",
    "# np.random.shuffle(questions)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup OpenAI Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatOpenAI(\n",
    "    model_name=wandb.config.eval_model,\n",
    "    temperature=wandb.config.openai_eval_llm_temperature,\n",
    "    request_timeout=120\n",
    "    )\n",
    "\n",
    "eval_chain = LLMChain(llm=eval_llm, \n",
    "                      prompt=eval_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get eval_prompt template token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval prompt template token count: 701\n"
     ]
    }
   ],
   "source": [
    "# CALCUALTE THE EMPTY EVAL PROMPT TEAMPLATE TOKEN COUNT\n",
    "openai_eval_prompt_template_txt = \"\"\n",
    "for m in eval_prompt.messages:\n",
    "    openai_eval_prompt_template_txt+=m.prompt.template\n",
    "\n",
    "enc = tiktoken.encoding_for_model(wandb.config.eval_model)\n",
    "eval_prompt_template_token_count = len(enc.encode(openai_eval_prompt_template_txt))\n",
    "print(f\"Eval prompt template token count: {eval_prompt_template_token_count}\")\n",
    "\n",
    "# REDUCE THE SOURCE DOCUMENTS TO FIT IN THE EVAL PROMPT\n",
    "def check_and_shorten_openai_eval_source_docs(question, answer, source_documents, \n",
    "                                              eval_prompt_template_token_count=eval_prompt_template_token_count,\n",
    "                                              max_eval_tokens=4096, completion_tokens=200):\n",
    "    '''\n",
    "    Sometimes the source docs are too long and we need to shorten them to fit in the eval prompt\n",
    "    max_eval_tokens : context length of the eval model\n",
    "    completion_tokens : if there is a need to reserve some tokens for the completion\n",
    "    '''\n",
    "\n",
    "    question_tokens = len(enc.encode(question))\n",
    "    answer_tokens = len(enc.encode(answer))\n",
    "    available_source_tokens = max_eval_tokens - (eval_prompt_template_token_count + question_tokens + answer_tokens) - completion_tokens\n",
    "\n",
    "    if available_source_tokens < 0:\n",
    "        source_documents = \"\"\n",
    "        source_documents_truncated = True\n",
    "        tokens_left_for_source_documents = 0\n",
    "        return source_documents, source_documents_truncated, tokens_left_for_source_documents\n",
    "    else:\n",
    "        source_docs_token_ids = enc.encode(source_documents)\n",
    "        source_docs_tokens = len(source_docs_token_ids)\n",
    "    \n",
    "        remaining_tokens = available_source_tokens - source_docs_tokens\n",
    "        if remaining_tokens < 0:\n",
    "            tokens_left_for_source_documents = available_source_tokens-10\n",
    "            source_documents = enc.decode(source_docs_token_ids[:tokens_left_for_source_documents])\n",
    "            source_documents_truncated = True\n",
    "        else:\n",
    "            source_documents_truncated = False\n",
    "            tokens_left_for_source_documents = source_docs_tokens\n",
    "        return source_documents, source_documents_truncated, tokens_left_for_source_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 701)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is wandb?\"\n",
    "answer = \"Weights & Biases is a machine learning platform for teams.\"\n",
    "source_documents = \"[hey, this is a document]\"*100\n",
    "\n",
    "_, source_documents_truncated, tokens_left_for_source_documents = check_and_shorten_openai_eval_source_docs(question, answer, source_documents)\n",
    "source_documents_truncated, tokens_left_for_source_documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Synth WandBot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup chain variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wandbot_v1_few_shot', 'wandbot_v1_zero_shot', 'default_langchain_qa'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = {}\n",
    "for system_prompt in system_prompts.keys():\n",
    "    chains[f\"{system_prompt}\"] = load_qa_chain(system_prompts[system_prompt], vector_store, chain_type=\"stuff\")\n",
    "\n",
    "chains.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import traceback\n",
    "import langchain; \n",
    "langchain.debug=False\n",
    "\n",
    "# for i_q, question in enumerate(questions):\n",
    "# def answer_and_grade(i_q, questions, chains, timestamps):\n",
    "def answer_and_grade(args):\n",
    "    ii, question_id, question, chains, tstamp = args\n",
    "\n",
    "    if (question == \"\") or (question is None):\n",
    "        return\n",
    "\n",
    "    # USER QUERY\n",
    "    request_id = generate_id(length=16)\n",
    "    \n",
    "    # RUN CHAIN\n",
    "    system_prompt = random.choice(list(chains.keys()))\n",
    "    qa_chain = chains[system_prompt]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        with get_openai_callback() as openai_cb:\n",
    "            response = qa_chain({\"question\": question}, \n",
    "                                callbacks=[WandbTracer(wandb_config)],\n",
    "                                return_only_outputs=False,\n",
    "                                )\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        answer = response[\"answer\"]\n",
    "\n",
    "        # RETRIEVED DOCUMENTS\n",
    "        source_docs = response[\"source_documents\"]\n",
    "        source_documents = parse_source_documents(source_docs)\n",
    "\n",
    "        # TOKEN METRICS\n",
    "        prompt_tokens = openai_cb.prompt_tokens\n",
    "        completion_tokens = openai_cb.completion_tokens\n",
    "        total_tokens = openai_cb.total_tokens\n",
    "        total_cost = openai_cb.total_cost\n",
    "        successful_requests = openai_cb.successful_requests\n",
    "\n",
    "        # GENERATE SYNTHETIC USER FEEDBACK       \n",
    "        try:     \n",
    "            eval_source_documents, is_source_documents_truncated, eval_tokens_left_for_source_documents = check_and_shorten_openai_eval_source_docs(question, answer, source_documents)\n",
    "\n",
    "            eval_start_time = time.time()\n",
    "            with get_openai_callback() as openai_eval_cb:\n",
    "                response = eval_chain({\n",
    "                            \"question\": question, \n",
    "                            \"answer\": answer, \n",
    "                            \"source_documents\": eval_source_documents},\n",
    "                            callbacks=[WandbTracer(wandb_config)],\n",
    "                            )\n",
    "                \n",
    "            eval_end_time = time.time()\n",
    "            eval_elapsed_time = eval_end_time - eval_start_time\n",
    "            eval_completion = response[\"text\"]\n",
    "\n",
    "            # EVAL TOKEN METRICS\n",
    "            eval_prompt_tokens = openai_eval_cb.prompt_tokens\n",
    "            eval_completion_tokens = openai_eval_cb.completion_tokens\n",
    "            eval_total_tokens = openai_eval_cb.total_tokens\n",
    "            eval_cost = openai_eval_cb.total_cost\n",
    "            eval_successful_requests = openai_eval_cb.successful_requests\n",
    "\n",
    "            grader_justification = eval_completion.split(\"GRADE:\")[0].strip()\n",
    "            if \"positive\" in eval_completion.lower():\n",
    "                grader_feedback = \"POSITIVE\"  \n",
    "            elif \"negative\" in eval_completion.lower():\n",
    "                grader_feedback = \"NEGATIVE\"\n",
    "            else:\n",
    "                grader_feedback = \"GRADER_FAILURE\"\n",
    "\n",
    "        except Exception as eval_e:\n",
    "            error_message = f\"Question {question_id}, Eval Error occured: {eval_e}\"\n",
    "            print(error_message)\n",
    "\n",
    "            eval_prompt_tokens = 0\n",
    "            eval_completion_tokens = 0\n",
    "            eval_total_tokens = 0\n",
    "            eval_cost = 0\n",
    "            eval_elapsed_time = 0\n",
    "            eval_completion = \"FAILURE\"\n",
    "            grader_feedback = \"GRADER_FAILURE\"\n",
    "            grader_justification = error_message\n",
    "\n",
    "            traceback.print_exc() \n",
    "\n",
    "        # LOG TO WANDB\n",
    "        wandb_table = wandb.Table(table_cols)\n",
    "        wandb_table.add_data(tstamp, request_id, question, question_id, answer, source_documents,\n",
    "                            elapsed_time, grader_feedback, grader_justification, # eval_completion,\n",
    "                            prompt_tokens, completion_tokens, total_tokens,\n",
    "                            total_cost, successful_requests, system_prompt,\n",
    "                            eval_elapsed_time, eval_cost, eval_total_tokens, eval_tokens_left_for_source_documents, eval_prompt_tokens, eval_completion_tokens, is_source_documents_truncated,\n",
    "                            # system_prompts[system_prompt], human_message_prompt_template, hyde_prompt_template, openai_eval_prompt_template_txt,\n",
    "                            wandb.run.id, wandb.config.model_name, wandb.config.temperature, wandb.config.hyde_llm_temperature, \n",
    "                            wandb.config.eval_model)\n",
    "        wandb.log({\"logs/qa_with_eval_final2\": wandb_table})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Question {question_id}, Error occured: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    if ii % 50 == 0: print(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_table_cols = [\"query_id\",\"run_id\",  \"system_prompt_template\", \n",
    "#                      \"human_message_prompt_template\", \"hyde_prompt_template\", \"eval_grader_prompt_template\"]\n",
    "# config_table = wandb.Table(config_table_cols)  \n",
    "# config_table.add_data(query_id, wandb.run.id, system_prompts[system_prompt], \n",
    "#                       human_message_prompt_template, hyde_prompt_template, eval_grader_prompt_template)\n",
    "# wandb.log({\"logs/config_table_test\": config_table})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run logging in parellal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of examples to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df[\"question\"].values.tolist()) #10000\n",
    "qs = df[\"question\"].values.tolist()[:n]\n",
    "q_ids = df[\"question_id\"].values.tolist()[:n]\n",
    "ts = timestamps_list[:n]\n",
    "\n",
    "n, len(qs), len(q_ids), len(ts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fastcore.parallel import parallel\n",
    "\n",
    "start = time.perf_counter()\n",
    "parallel(answer_and_grade, list(zip(list(range(len(qs))), q_ids, qs, [chains]*len(qs), ts)), \n",
    "         n_workers=7, progress=True)  \n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f'Time taken for {n} questions: {end - start} seconds, {(end - start)/n} seconds per question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res  = answer_and_grade((1, q_ids[0], qs[0], chains, ts[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log additional reference tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts_cols = [\"run_id\", \"system_prompt_version\", \"system_prompt\"]\n",
    "system_prompts_table = wandb.Table(system_prompts_cols)  \n",
    "for c in chains.keys():\n",
    "    system_prompts_table.add_data(wandb.run.id, c, system_prompts[c])\n",
    "wandb.log({\"logs/answer_prompt_versions\": system_prompts_table})\n",
    "\n",
    "other_prompts_cols = [\"run_id\", \"eval_grader_prompt_template\", \"human_message_prompt_template\"]\n",
    "other_prompts_table = wandb.Table(other_prompts_cols)  \n",
    "for c in chains.keys():\n",
    "    other_prompts_table.add_data(wandb.run.id, human_message_prompt_template, hyde_prompt_template, openai_eval_prompt_template_txt)\n",
    "wandb.log({\"logs/other_prompts\": other_prompts_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">synth_answer_generation_test_13</strong> at: <a href='https://wandb.ai/wandbot/wandbot_synth/runs/su5lnq02' target=\"_blank\">https://wandb.ai/wandbot/wandbot_synth/runs/su5lnq02</a><br/>Synced 6 W&B file(s), 12 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_112408-su5lnq02/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WandbTracer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "### Unused Prompts\n",
    "Cohere Command Grader Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_prompt = \"Human:\"\n",
    "# assistant_prompt = \"GRADER_RESPONSE\"\n",
    "\n",
    "# grade_command = \"\"\"Grade the following WANDBOT_RESPONSE given the USER_QUESTION and SUPPORTING_DOCUMENTATION. \n",
    "# Grade the WANDBOT_RESPONSE based ONLY on its factual accuracy. It is OK if the WANDBOT_RESPONSE contains more information than in SUPPORTING_DOCUMENTATION, as long as it does not contain any conflicting statements.\n",
    "# Your GRADE should only be POSITIVE or NEGATIVE to indicate whether the WANDBOT_RESPONSE is accurate or not given the SUPPORTING_DOCUMENTATION.\n",
    "# If the WANDBOT_RESPONSE is that there is no specific information provided in the context or that it doesn't know, then the GRADE is NEGATIVE.\n",
    "# Be sure to read the SUPPORTING_DOCUMENTATION carefully before grading the WANDBOT_RESPONSE and be highlighly critical of WANDBOT_RESPONSE.\n",
    "# Only respond with POSITIVE or NEGATIVE for GRADE. \n",
    "# In GRADE_JUSTIFICATION, explain why your GRADE of WANDBOT_RESPONSE is POSITIVE or NEGATIVE. Use one or two sentences maximum. Keep the answer as concise as possible.\"\"\"\n",
    "\n",
    "\n",
    "# def command_eval_prompt_constructor(question, source_documents, answer, grade_command=grade_command):\n",
    "#     evaluation_prompts_template = f\"\"\"As an experienced software quality assurance tester, you are evaluating the quality of the response (WANDBOT_RESPONSE) from a\n",
    "# Weights & Biasses (aka wandb, W&B) support bot called `wandbot`. Weights & Biasses is a machine learning ops (MLOps) python library and app.\n",
    "# Supporting documentation (SUPPORTING_DOCUMENTATION) is provided to help you assess the quality of the response. You job is to grade (GRADE) the response.\n",
    "\n",
    "# This is the example format of the input and a grade given to the `wandbot` support bot response:\n",
    "\n",
    "# =====================\n",
    "# USER_QUESTION: user question here\n",
    "# WANDBOT_RESPONSE: the response from the `wandbot` support bot here\n",
    "# SUPPORTING_DOCUMENTATION: retrieved documentation from the wandb docs here\n",
    "\n",
    "# {assistant_prompt}\n",
    "# GRADE_JUSTIFICATION: justification for the GRADE here\n",
    "# GRADE: POSITIVE or NEGATIVE here\n",
    "# =====================\n",
    "\n",
    "# this is a real example:\n",
    "\n",
    "# =====================\n",
    "# USER_QUESTION: How do I create a wandb sweep?\n",
    "# WANDBOT_RESPONSE: To create a W&B Articfact, you can use the wandb.Artifact class like so ```artifact = wandb.Artifact(name='bicycle-dataset', type='dataset')```\n",
    "# SUPPORTING_DOCUMENTATION: '{{\"source_doc_0\": {{\"page_content\": \"Use Weights & Biases Sweeps to automate hyperparameter search and explore the space of possible models. Create a sweep with a few lines of code.\", \"metadata\": \"https://docs.wandb.ai/guide\", \"lookup_index\": 0, \"lookup_str\": \"\"}}}}'\n",
    "\n",
    "# {assistant_prompt}\n",
    "# GRADE_JUSTIFICATION: The USER_QUESTION was about how to create a wandb sweep, but the WANDBOT_RESPONSE answers how to create a wandb artifact. The WANDBOT_RESPONSE is not accurate.\n",
    "# GRADE: NEGATIVE\n",
    "# =====================\n",
    "\n",
    "# {grade_command}\n",
    "\n",
    "# =====================\n",
    "# USER_QUESTION: {question}\n",
    "# WANDBOT_RESPONSE: {answer}\n",
    "# SUPPORTING_DOCUMENTATION: {source_documents}\n",
    "\n",
    "# {assistant_prompt}\n",
    "# GRADE_JUSTIFICATION:\"\"\"\n",
    "#     return evaluation_prompts_template\n",
    "\n",
    "# question = \"what is wandb?\"\n",
    "# answer = \"Weights & Biases is a machine learning platform for teams.\"\n",
    "# source_documents = \"[hey, this is a document]\"\n",
    "# print(command_eval_prompt_constructor(question, source_documents, answer, grade_command))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere Command prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "\n",
    "# eval_grader_prompt_template = command_eval_prompt_constructor(\"\", \"\", \"\")\n",
    "\n",
    "# command_nightly_tokenizer = Tokenizer.from_pretrained(\"Cohere/command-nightly\")\n",
    "# prompt_enc = command_nightly_tokenizer.encode(eval_grader_prompt_template)\n",
    "# print(f\"Command prompt template token count: {len(prompt_enc.ids)}, character count: {len(eval_grader_prompt_template)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude Grader Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def claude_eval_prompt_constructor(question, source_documents, answer):\n",
    "#     evaluation_prompts_template = f\"\"\"As an experienced software quality assurance tester, you are evaluating the quality of the response from a\n",
    "#     Weights & Biasses (aka wandb, W&B) support bot called wandbot. Weights & Biasses is a machine learning ops (MLOps) python library and app.\n",
    "#     Supporting documentation is provided to help you assess the quality of the response.\n",
    "#     Your feedback should only only be \"POSITIVE\" or \"NEGATIVE\" to indicate whether the response is accurate or not,\n",
    "#     no other information is required. For example:\n",
    "#     {anthropic.HUMAN_PROMPT}\n",
    "#     =====================\n",
    "#     USER_QUESTION: What is wandb?\n",
    "#     SUPPORTING_DOCUMENTATION: '{{\"source_doc_0\": {{\"page_content\": \"Weights & Biases is the machine learning platform for developers to build better models faster\", \"metadata\": \"https://docs.wandb.ai/guide\", \"lookup_index\": 0, \"lookup_str\": \"\"}}}}'\n",
    "#     WANDBOT_RESPONSE: Weights & Biases is a machine learning platform for teams.\n",
    "#     {anthropic.AI_PROMPT}\n",
    "#     POSITIVE\n",
    "#     {anthropic.HUMAN_PROMPT}\n",
    "#     =====================\n",
    "#     USER_QUESTION: How do I create a wandb sweep?\n",
    "#     SUPPORTING_DOCUMENTATION: '{{\"source_doc_0\": {{\"page_content\": \"Use Weights & Biases Sweeps to automate hyperparameter search and explore the space of possible models. Create a sweep with a few lines of code.\", \"metadata\": \"https://docs.wandb.ai/guide\", \"lookup_index\": 0, \"lookup_str\": \"\"}}}}'\n",
    "#     WANDBOT_RESPONSE: To create a W&B Articfact, you can use the wandb.Artifact class like so ```artifact = wandb.Artifact(name='bicycle-dataset', type='dataset')```\n",
    "#     {anthropic.AI_PROMPT}\n",
    "#     NEGATIVE\n",
    "#     {anthropic.HUMAN_PROMPT}\n",
    "#     =====================\n",
    "#     USER_QUESTION: {question}\n",
    "#     SUPPORTING_DOCUMENTATION: {source_documents}\n",
    "#     WANDBOT_RESPONSE: {answer}\n",
    "#     {anthropic.AI_PROMPT}\"\"\"\n",
    "#     return evaluation_prompts_template\n",
    "\n",
    "# question = \"what is wandb?\"\n",
    "# answer = \"Weights & Biases is a machine learning platform for teams.\"\n",
    "# print(claude_eval_prompt_constructor(question, source_documents, answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Cohere Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cohere\n",
    "\n",
    "# def calculate_eval_tokens(eval_grader_prompt, eval_completion):\n",
    "#     prompt_enc = command_nightly_tokenizer.encode(eval_grader_prompt)\n",
    "#     prompt_tokens_count = len(prompt_enc.ids)\n",
    "#     completion_enc = command_nightly_tokenizer.encode(eval_completion)\n",
    "#     completion_token_count = len(completion_enc.ids)\n",
    "#     completion_total_tokens = prompt_tokens_count + completion_token_count\n",
    "#     return completion_total_tokens, prompt_tokens_count, completion_token_count\n",
    "\n",
    "\n",
    "# def check_and_shorten_eval_source_docs(question, answer, source_documents, grade_command, max_eval_tokens=4096):\n",
    "#     '''\n",
    "#     Sometimes the source docs are too long and we need to shorten them to fit in the eval prompt\n",
    "#     '''\n",
    "#     dummy_eval_prompt = command_eval_prompt_constructor(question, \"\", answer, grade_command)\n",
    "\n",
    "#     dummy_eval_prompt_enc = command_nightly_tokenizer.encode(dummy_eval_prompt)\n",
    "#     available_eval_tokens = max_eval_tokens - len(dummy_eval_prompt_enc.ids) \n",
    "\n",
    "#     source_docs_enc = command_nightly_tokenizer.encode(source_documents)\n",
    "#     source_docs_tokens = len(source_docs_enc.ids) \n",
    "\n",
    "#     remainder_eval_tokens = available_eval_tokens - source_docs_tokens\n",
    "#     if remainder_eval_tokens < 0:\n",
    "#         source_documents = command_nightly_tokenizer.decode(source_docs_enc.ids[:available_eval_tokens-50])\n",
    "#         is_source_documents_truncated = True\n",
    "#     else: is_source_documents_truncated = False\n",
    "\n",
    "#     return source_documents, is_source_documents_truncated\n",
    "\n",
    "# co = cohere.Client(cohere_api_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ANTHROPIC EVALUATION\n",
    "# eval_model = \"claude-v1.3-100k\" # \"claude-v1\",\n",
    "# # anthropic_api = \"XXX\"\n",
    "# # client = anthropic.Client(api_key=anthropic_api)\n",
    "# # max_tokens_to_sample = 100000\n",
    "\n",
    "# eval_prompt_template = claude_eval_prompt_constructor(\"\", \"\", \"\")  # Just to log the eval prompt template\n",
    "# # eval_prompt = claude_eval_prompt_constructor(question, source_documents, answer)\n",
    "\n",
    "# eval_start_time = time.time()\n",
    "# # resp = client.completion(\n",
    "# #     prompt=eval_prompt,\n",
    "# #     stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "# #     model=eval_model,\n",
    "# #     max_tokens_to_sample=max_tokens_to_sample,\n",
    "# # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
