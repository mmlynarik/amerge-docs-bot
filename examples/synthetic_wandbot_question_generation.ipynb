{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will automatically generate a set of evaluation questions based on wandb docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import openai\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.openai import autolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"langchain>=0.0.175\" \"wandb>=0.15.3\" openai cohere tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a repo with documentation pages saved as .md files\n",
    "# !git clone https://github.com/wandb/docodile.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Weights & Biases Project and Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"wandbot_synth\" \n",
    "ENTITY = \"wandbot\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter password in the VS Code prompt at the top of your VS Code window!\n",
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "def get_openai_key():\n",
    "  if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "      print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "  assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "  print(\"OpenAI API key configured\")\n",
    "\n",
    "get_openai_key()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic User Questions using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to find all the markdown files in a directory and return it's content and path\n",
    "\n",
    "def find_md_files(directory):\n",
    "    md_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as md_file:\n",
    "                    content = md_file.read()\n",
    "                md_files.append((file_path, content))\n",
    "    return md_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = find_md_files('../docodile/docs/')\n",
    "\n",
    "random.shuffle(documents)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a tester for a support bot for the Weights & Biases (aka wandb or W&B) MLOps python library.'\n",
    "    'Your goal is to generate 40 questions of varying difficulty that can be answered by reading a document in the wandb documentation.'\n",
    "    'Given a document, you need to imagine a hypothetical question from a user that has not read the document before.\n",
    "    'It should be possible to answer the question by reading and reasoning over the document.\n",
    "    'This question should be feasible to come from a user learning about wandb.\n",
    "    'The question should be answerable by a human.\n",
    "    'Each question should be unique and not a duplicate of another question.\n",
    "    'Each question should be separated by a new line.\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Autolog\n",
    "autolog({\"project\":PROJECT, \"entity\":ENTITY, \"name\":\"synth_question_generation\"})\n",
    "\n",
    "res = []\n",
    "for i in tqdm(range(len(documents))):\n",
    "    for i_r, _ in enumerate(range(5)):\n",
    "        try: \n",
    "            source = documents[i][0]\n",
    "            doc = documents[i][1]\n",
    "            generation_prompt = f'''Let's start!\n",
    "                Please generate 40 questions of varying difficulty that can be answered by reading this document from the wandb documentation;\n",
    "                Document: {doc}\n",
    "                Questions:'''\n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": generation_prompt},\n",
    "                    ]\n",
    "            )\n",
    "            generation = response.choices[0].message.content\n",
    "            res.append({\n",
    "                'prompt': generation_prompt,\n",
    "                'document': source,\n",
    "                'question': generation\n",
    "            })\n",
    "            # we don't need to retry if we get here\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"This model's maximum context length is 4097 tokens\" in str(e): break\n",
    "            # wait for before retrying\n",
    "            time.sleep(10)\n",
    "            print(f'retrying {i}: {e}')\n",
    "\n",
    "    # if i == 2: break # for testing\n",
    "    if i % 20 == 0: print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process\n",
    "Post-process openai completitions into structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(f\"{res[:3]}\\n\")\n",
    "\n",
    "# we will now split each generation in res into a list of questions based on the new line character and flatten the resulting list\n",
    "res = [{'prompt': x['prompt'], 'document': x['document'], 'question': y} for x in res for y in x['question'].split('\\n')]\n",
    "\n",
    "# now let's remove the numeric characters, point and space at the beginning of each question\n",
    "for i in range(len(res)):\n",
    "    res[i]['question'] = re.sub(r'^[\\d. ]+', '', res[i]['question'])\n",
    "\n",
    "\n",
    "qs = [x['question'] for x in res]\n",
    "for i,q in enumerate(qs):\n",
    "    # print(q)\n",
    "    if i > 20: break\n",
    "    \n",
    "# Save to DataFrame and CSV\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv('sythetic-user-questions_2023-05-16.csv', index=False)\n",
    "print(f\"{df.head()}\\n\")\n",
    "print(f\"\\n{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to wandb\n",
    "wandb.log({'generated_questions_table': wandb.Table(dataframe=df)})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
